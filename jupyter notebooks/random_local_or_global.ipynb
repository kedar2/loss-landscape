{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ea39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from cvxopt import matrix, solvers\n",
    "solvers.options['show_progress'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de4a382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, d0, d1, d2, freeze=False):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        lin_layer1 = nn.Linear(d0, d1)        \n",
    "        torch.nn.init.normal_(lin_layer1.bias, mean=0., std=np.sqrt(2. / d0))        \n",
    "        torch.nn.init.kaiming_normal_(lin_layer1.weight, nonlinearity='relu')\n",
    "        if freeze:\n",
    "            lin_layer1.bias.requires_grad = False\n",
    "            lin_layer1.weight.requires_grad = False\n",
    "        layers.append(lin_layer1)\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        lin_layer2 = nn.Linear(d1, d2, bias=False)\n",
    "        torch.nn.init.normal_(lin_layer2.weight, mean=0., std=np.sqrt(1. / d1))\n",
    "        # Freeze the weights in the last layer\n",
    "        lin_layer2.weight.requires_grad = False\n",
    "        layers.append(lin_layer2)\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "def get_gaussian_data(d0, data_size, target_fn):\n",
    "    x = torch.tensor(np.random.normal(size=(data_size, d0)), dtype=torch.float)\n",
    "    y = target_fn(x)\n",
    "    return x, y\n",
    "\n",
    "def get_A(model, x):\n",
    "    return model.layers[0](x).detach().numpy() > 0\n",
    "\n",
    "def contains_min(model, x, y):    \n",
    "    N = x.size()[0]\n",
    "    d1 = len(model.layers[0].weight)\n",
    "    \n",
    "    out1 = model.layers[0](x).detach().numpy()    \n",
    "    pattern = out1 > 0\n",
    "    \n",
    "    w = model.layers[0].weight.detach().numpy().astype(np.float64)\n",
    "    b = model.layers[0].bias.detach().numpy().astype(np.float64)\n",
    "    v = model.layers[2].weight.detach().numpy().astype(np.float64)\n",
    "    x_np = x.detach().numpy().astype(np.float64)\n",
    "    y_np = y.detach().numpy().astype(np.float64)\n",
    "    \n",
    "    # Check the number of dead neurons\n",
    "    false_dict = defaultdict(int)\n",
    "    for x_pattern in pattern:\n",
    "        for pi, p in enumerate(x_pattern):\n",
    "            if not p:\n",
    "                false_dict[pi] += 1\n",
    "    dead_id = [ni for ni in false_dict if false_dict[ni] == N]\n",
    "    \n",
    "    # Construct x for the linear regression problem\n",
    "    alive_v = np.asarray([[one_v for vi, one_v in enumerate(v[0]) if vi not in dead_id]])\n",
    "    alive_pattern = np.asarray([\n",
    "        [one_p for pi, one_p in enumerate(p_row) if pi not in dead_id] for p_row in pattern])\n",
    "    masked_v = np.concatenate([alive_v for _ in range(N)])\n",
    "    masked_v[np.invert(alive_pattern)] = 0.\n",
    "     \n",
    "    masked_vx = alive_v * x_np\n",
    "    masked_vx[np.invert(alive_pattern)] = 0.\n",
    "    \n",
    "    x_tilde = np.concatenate((masked_v, masked_vx), axis=1)\n",
    "    \n",
    "    ########################################################\n",
    "    ########################################################\n",
    "    ########################################################\n",
    "    # Find the quadratic problem solution\n",
    "    P = matrix(x_tilde.T @ x_tilde)\n",
    "    q = matrix(- x_tilde.T @ y_np)\n",
    "    param_num = P.size[1]  \n",
    "    \n",
    "    G = np.zeros((N * param_num // 2, param_num))\n",
    "    for xi, x_pattern in enumerate(alive_pattern):\n",
    "        for pi, param_pattern in enumerate(x_pattern):\n",
    "            # wx + b > 0\n",
    "            if param_pattern:\n",
    "                G[xi * param_num // 2 + pi][pi] = -1\n",
    "                G[xi * param_num // 2 + pi][param_num // 2 + pi] = -x_np[xi]\n",
    "            # wx + b <= 0\n",
    "            else:\n",
    "                G[xi * param_num // 2 + pi][pi] = 1\n",
    "                G[xi * param_num // 2 + pi][param_num // 2 + pi] = x_np[xi]\n",
    "    G = matrix(G)\n",
    "    h = matrix(np.zeros(N * param_num // 2))\n",
    "    \n",
    "    beta_hat = np.array(solvers.qp(P, q, G, h)['x'])\n",
    "    ########################################################\n",
    "    ########################################################\n",
    "    ########################################################\n",
    "    \n",
    "#     # Find the linear regression solution\n",
    "#     beta_hat, _, _, _ = np.linalg.lstsq(x_tilde, y_np, rcond=None)\n",
    "        \n",
    "    # Check loss\n",
    "    pred_y = x_tilde @ beta_hat    \n",
    "    loss = np.mean((pred_y - y_np)**2)\n",
    "    zero_loss = np.isclose(loss, 0)\n",
    "        \n",
    "    # Check activation pattern of the found solution\n",
    "    new_weight = w.copy()\n",
    "    new_bias = b.copy()\n",
    "    dead_count = 0\n",
    "    for ni in range(d1):\n",
    "        if ni not in dead_id:\n",
    "            new_bias[ni] = beta_hat[ni - dead_count][0]\n",
    "            new_weight[ni] = beta_hat[d1 - len(dead_id) + ni - dead_count]\n",
    "        else:\n",
    "            dead_count += 1\n",
    "    \n",
    "    new_out = x_np @ new_weight.T + new_bias   \n",
    "    new_pattern = new_out > 0\n",
    "    \n",
    "    same_pattern = tuple(pattern.reshape(-1)) == tuple(new_pattern.reshape(-1))\n",
    "        \n",
    "    parameter_dim = len(model.layers[0].weight) * 2\n",
    "    eq_num = np.sum(np.isclose(np.min(np.abs(out1), axis=-1), 0))\n",
    "    region_dim = parameter_dim - eq_num\n",
    "        \n",
    "    return loss, zero_loss, same_pattern, region_dim, new_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6508cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! d1: 100\n",
      "!!! data_size: 100\n",
      "=== Run 100/100 ===\n",
      "Number of global minima: 0/100\n",
      "Number of same patterns: 100/100\n",
      "Unique lr patterns: 100/100\n",
      "Average region dimension: 200.0\n",
      "\n",
      "!!! data_size: 200\n",
      "=== Run 100/100 ===\n",
      "Number of global minima: 0/100\n",
      "Number of same patterns: 100/100\n",
      "Unique lr patterns: 100/100\n",
      "Average region dimension: 200.0\n",
      "\n",
      "!!! data_size: 300\n"
     ]
    }
   ],
   "source": [
    "RUNS_NUM = 100\n",
    "\n",
    "d1_arr = [100 * (i+ 1) for i in range(5)]#20)]\n",
    "data_size_arr = [100 * (i+ 1) for i in range(5)]#20)]\n",
    "\n",
    "total_zero_loss = []\n",
    "total_same_pattern = []\n",
    "total_region_dim = []\n",
    "\n",
    "for d1 in d1_arr:\n",
    "    print(f'!!! d1: {d1}')\n",
    "    d1_zero_loss = []\n",
    "    d1_same_pattern = []\n",
    "    d1_region_dim = []\n",
    "    for data_size in data_size_arr:\n",
    "        print(f'!!! data_size: {data_size}')\n",
    "        teacher_net = TwoLayerNet(d0=1, d1=d1, d2=1, freeze=True)\n",
    "        teacher_net.train(False)\n",
    "        x, y = get_gaussian_data(d0=1, data_size=data_size, target_fn=teacher_net)\n",
    "        \n",
    "        original_pattern_arr = []\n",
    "        same_pattern_arr = np.asarray([False for _ in range(RUNS_NUM)])\n",
    "        region_dim_arr = np.zeros(RUNS_NUM)\n",
    "        zero_loss_arr = np.asarray([False for _ in range(RUNS_NUM)])\n",
    "        lr_pattern_arr = []\n",
    "\n",
    "        run_id = 0\n",
    "        while len(original_pattern_arr) < RUNS_NUM:\n",
    "            if (run_id + 1) % 100 == 0:\n",
    "                print(f'=== Run {run_id + 1}/{RUNS_NUM} ===')\n",
    "            student_net = TwoLayerNet(d0=1, d1=d1, d2=1) \n",
    "            pattern_hash = hash(tuple(get_A(student_net, x).reshape(-1)))\n",
    "            if pattern_hash not in original_pattern_arr:\n",
    "                original_pattern_arr.append(pattern_hash)\n",
    "                (_, zero_loss_arr[run_id], same_pattern_arr[run_id],\n",
    "                 region_dim_arr[run_id], lr_pattern) = contains_min(student_net, x, y)\n",
    "                lr_pattern_arr.append(hash(tuple(lr_pattern.reshape(-1))))\n",
    "                run_id +=1\n",
    "\n",
    "        print(f'Number of global minima: {np.sum(zero_loss_arr)}/{RUNS_NUM}')\n",
    "        print(f'Number of same patterns: {np.sum(same_pattern_arr)}/{RUNS_NUM}')\n",
    "        print(f'Unique lr patterns: {np.unique(lr_pattern_arr).shape[0]}/{RUNS_NUM}')\n",
    "        print(f'Average region dimension: {np.mean(region_dim_arr)}')\n",
    "        print()\n",
    "        \n",
    "        d1_zero_loss.append(np.sum(zero_loss_arr) / RUNS_NUM * 100)\n",
    "        d1_same_pattern.append(np.sum(same_pattern_arr) / RUNS_NUM * 100)\n",
    "        d1_region_dim.append(np.mean(region_dim_arr))\n",
    "        \n",
    "    total_zero_loss.append(d1_zero_loss)\n",
    "    total_same_pattern.append(d1_same_pattern)\n",
    "    total_region_dim.append(d1_region_dim)\n",
    "    \n",
    "total_zero_loss = np.asarray(total_zero_loss)\n",
    "print(f'total_zero_loss:\\n{total_zero_loss.shape}')\n",
    "print(f'total_zero_loss:\\n{total_zero_loss}')\n",
    "print(f'total_same_pattern:\\n{total_same_pattern}')\n",
    "print(f'total_region_dim:\\n{total_region_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc47532",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 5\n",
    "\n",
    "colormap_arr = ['RdYlBu_r']\n",
    "\n",
    "for colormap in colormap_arr:\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    fig = plt.figure(figsize=(10.7, 8), dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=36)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=36)\n",
    "\n",
    "    plt.xlabel('Data size', size=40)\n",
    "    plt.ylabel('Network width', size=40)\n",
    "    plt.margins(x=0)\n",
    "\n",
    "    plt.xticks(list(range(0, len(data_size_arr) , STEP)) + [len(data_size_arr) - 1],\n",
    "               [d for di, d in enumerate(data_size_arr) if di % STEP == 0] + [data_size_arr[-1]])\n",
    "    plt.yticks(list(range(0, len(d1_arr), STEP)) + [len(d1_arr) - 1],\n",
    "               [d for di, d in enumerate(d1_arr) if di % STEP == 0] + [d1_arr[-1]])\n",
    "\n",
    "    cp = plt.imshow(total_zero_loss, cmap=colormap, origin='lower', interpolation='nearest')\n",
    "    cbar = fig.colorbar(cp)\n",
    "\n",
    "    cbar.ax.tick_params(labelsize=36)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f'images/random_local_or_global/{timestamp}_global_percentage_{colormap}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f038a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
